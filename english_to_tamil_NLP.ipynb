{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiXxH6ACKnMv",
        "outputId": "15978a61-0819-41ed-aa44-279729bb7b39"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== SINGLE CELL: English → Tamil (LSTM Seq2Seq) ==================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ------------------ DATA ------------------\n",
        "eng_data = [\n",
        "    \"hello\",\n",
        "    \"good morning\",\n",
        "    \"thank you\",\n",
        "    \"how are you\",\n",
        "    \"i am learning ai\",\n",
        "    \"good night\"\n",
        "]\n",
        "\n",
        "tam_data = [\n",
        "    \"START வணக்கம் END\",\n",
        "    \"START காலை வணக்கம் END\",\n",
        "    \"START நன்றி END\",\n",
        "    \"START நீங்கள் எப்படி இருக்கிறீர்கள் END\",\n",
        "    \"START நான் ai கற்றுக்கொள்கிறேன் END\",\n",
        "    \"START இனிய இரவு END\"\n",
        "]\n",
        "\n",
        "# ------------------ TOKENIZATION ------------------\n",
        "eng_tok = Tokenizer()\n",
        "tam_tok = Tokenizer()\n",
        "\n",
        "eng_tok.fit_on_texts(eng_data)\n",
        "tam_tok.fit_on_texts(tam_data)\n",
        "\n",
        "eng_seq = eng_tok.texts_to_sequences(eng_data)\n",
        "tam_seq = tam_tok.texts_to_sequences(tam_data)\n",
        "\n",
        "max_eng = max(len(s) for s in eng_seq)\n",
        "max_tam = max(len(s) for s in tam_seq)\n",
        "\n",
        "enc_input = pad_sequences(eng_seq, maxlen=max_eng, padding=\"post\")\n",
        "dec_input = pad_sequences(tam_seq, maxlen=max_tam, padding=\"post\")\n",
        "\n",
        "dec_target = np.zeros_like(dec_input)\n",
        "dec_target[:, :-1] = dec_input[:, 1:]\n",
        "\n",
        "# ------------------ MODEL ------------------\n",
        "eng_vocab = len(eng_tok.word_index) + 1\n",
        "tam_vocab = len(tam_tok.word_index) + 1\n",
        "\n",
        "embed_dim = 64\n",
        "latent_dim = 128\n",
        "\n",
        "# Encoder\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(eng_vocab, embed_dim)(enc_inputs)\n",
        "_, h, c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
        "enc_states = [h, c]\n",
        "\n",
        "# Decoder\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb = Embedding(tam_vocab, embed_dim)(dec_inputs)\n",
        "dec_out, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True)(\n",
        "    dec_emb, initial_state=enc_states\n",
        ")\n",
        "dec_dense = Dense(tam_vocab, activation=\"softmax\")\n",
        "dec_out = dec_dense(dec_out)\n",
        "\n",
        "# Training model\n",
        "model = Model([enc_inputs, dec_inputs], dec_out)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "model.fit(\n",
        "    [enc_input, dec_input],\n",
        "    dec_target[..., np.newaxis],\n",
        "    epochs=400,\n",
        "    batch_size=2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# ------------------ INFERENCE ------------------\n",
        "encoder_model = Model(enc_inputs, enc_states)\n",
        "\n",
        "state_h = Input(shape=(latent_dim,))\n",
        "state_c = Input(shape=(latent_dim,))\n",
        "states_inputs = [state_h, state_c]\n",
        "\n",
        "dec_out_inf, h_inf, c_inf = model.layers[5](\n",
        "    dec_emb, initial_state=states_inputs\n",
        ")\n",
        "dec_out_inf = dec_dense(dec_out_inf)\n",
        "decoder_model = Model(\n",
        "    [dec_inputs] + states_inputs,\n",
        "    [dec_out_inf, h_inf, c_inf]\n",
        ")\n",
        "\n",
        "reverse_tam = {v: k for k, v in tam_tok.word_index.items()}\n",
        "\n",
        "def translate(sentence):\n",
        "    seq = eng_tok.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_eng, padding=\"post\")\n",
        "    states = encoder_model.predict(seq, verbose=0)\n",
        "\n",
        "    target = np.zeros((1, 1))\n",
        "    target[0, 0] = tam_tok.word_index[\"start\"]\n",
        "\n",
        "    result = []\n",
        "\n",
        "    for _ in range(max_tam):\n",
        "        preds, h, c = decoder_model.predict([target] + states, verbose=0)\n",
        "        idx = np.argmax(preds[0, -1])\n",
        "        word = reverse_tam.get(idx, \"\")\n",
        "\n",
        "        if word == \"end\":\n",
        "            break\n",
        "\n",
        "        result.append(word)\n",
        "        target[0, 0] = idx\n",
        "        states = [h, c]\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# ------------------ TEST ------------------\n",
        "print(\"English:\", \"good morning\")\n",
        "print(\"Tamil:\", translate(\"good morning\"))\n",
        "\n",
        "print(\"English:\", \"hello\")\n",
        "print(\"Tamil:\", translate(\"hello\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVLut4feJ8BX",
        "outputId": "846074e3-8c24-403c-f94f-fa1c3cfc5a6e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: good morning\n",
            "Tamil: காலை வணக்கம்\n",
            "English: hello\n",
            "Tamil: வணக்கம்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPCUtE0pKlo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6DaWzfGKIUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}